{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Consumer Finance Complaints\n",
    "This notebook is a quick analysis of the US Consumer Finance Complaints dataset. The dataset is available on [Kaggle](https://www.kaggle.com/datasets/kaggle/us-consumer-finance-complaints) and contains complaints about consumer financial products and services that were sent to companies for response. The dataset contains 18 columns and 555957 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import stat\n",
    "from IPython.display import display\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "First, we need to import the dataset. We will use the `pandas` library to read the dataset into a `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the file\n",
    "\n",
    "filename = 'data/Consumer_Complaints.csv'\n",
    "\n",
    "file = stat(filename)\n",
    "print(f'File size: {file.st_size / 1024 / 1024} MB.')\n",
    "\n",
    "# Read the CSV file\n",
    "\n",
    "df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "print(f'Data from the CSV file: {df.shape}')\n",
    "display(df.info())\n",
    "display(df.head(10))\n",
    "display(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Some values are missing.\n",
    "We are going to drop the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of NA: {df.isna().sum().sum()}.\")\n",
    "\n",
    "print(f\"Before removing NA: {df.shape}.\")\n",
    "df.dropna(axis = 0, how = 'any', inplace = True)\n",
    "print(f\"After removing NA: {df.shape}.\")\n",
    "display(df.info())\n",
    "display(df.head(10))\n",
    "display(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will convert the `date_received` column to a `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date to a datetime object\n",
    "df['date_received'] = pd.to_datetime(df['date_received'])\n",
    "df['date_sent_to_company'] = pd.to_datetime(df['date_sent_to_company'])\n",
    "\n",
    "company_dict = {}\n",
    "\n",
    "df.drop(['complaint_id'], axis=1, inplace=True)\n",
    "\n",
    "for p in [\"consumer_complaint_narrative\",'company_public_response','product', 'sub_product', 'issue', 'sub_issue', 'company', 'state', 'zipcode', 'tags', 'consumer_consent_provided', 'submitted_via', 'company_response_to_consumer', 'timely_response', 'consumer_disputed?']:\n",
    "    i = 0\n",
    "    dic = {}\n",
    "    for k in df[p].unique():\n",
    "        dic[k] = i\n",
    "        i+=1\n",
    "    df[p] = df[p].map(dic)\n",
    "    if p == 'company' and len(company_dict) == 0:\n",
    "        company_dict = dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot the number of complaints per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.hist(df['date_received'], bins=10, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's plot the number of complaints by company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of companies: {df['company'].nunique()}.\")\n",
    "print(f\"Matching between numbers and companies: {company_dict}.\")\n",
    "plt.figure(figsize=(15,9))\n",
    "plt.hist(df['company'], bins=df['company'].nunique(), color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to plot the number of complaints by company for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in df[\"date_received\"].dt.year.unique():\n",
    "    plt.figure(figsize=(15,9))\n",
    "    plt.hist(df[df[\"date_received\"].dt.year == year][\"company\"], bins=df['company'].nunique(), color='blue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f\"Number of complaints per company in {year}\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing for the `subproduct` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sp in df['sub_product'].unique():\n",
    "    plt.figure(figsize=(15,9))\n",
    "    plt.hist(df[df['sub_product'] == sp]['company'], bins=df['company'].nunique(), color='blue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f\"Number of complaints per company for subproduct {sp}\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "### Linear model\n",
    "We are trying to predict the `consumer_disputed?` column, using all the other columns as features.\n",
    "As reminder, here are the names of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `consumer_disputed?` column is a boolean column, so we will use a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['product', 'sub_product', 'issue', 'sub_issue', 'company', 'state', 'zipcode', 'tags', 'consumer_consent_provided', 'submitted_via', 'company_response_to_consumer', 'timely_response']]\n",
    "Y = df['consumer_disputed?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "reg.fit(X_train, Y_train)\n",
    "print(f\"Score: {reg.score(X_test, Y_test)}.\")\n",
    "cvs = cross_val_score(reg, X, Y, cv=5)\n",
    "print(f\"Cross-validation score: {cvs}.\")\n",
    "print(f\"Mean cross-validation score: {cvs.mean()}.\")\n",
    "print(f\"Standard deviation of cross-validation score: {cvs.std()}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a good score, however, it seems not to converge very well.\n",
    "We are going to try other ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "We are going to use a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 100, max_depth=5, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"Score: {clf.score(X_test, Y_test)}.\")\n",
    "cvs = cross_val_score(clf, X, Y, cv=5)\n",
    "print(f\"Cross-validation score: {cvs}.\")\n",
    "print(f\"Mean cross-validation score: {cvs.mean()}.\")\n",
    "print(f\"Standard deviation of cross-validation score: {cvs.std()}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier seems to have a good score. Let's try to improve it.\n",
    "Perhaps we can use a grid search to find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [50,100,150],\n",
    "    'max_depth' : [2,3,4,5,6],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(f\"Score: {clf.score(X_test, Y_test)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
