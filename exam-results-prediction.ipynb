{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam results prediction\n",
    "This notebook will provide an analysis of the data and will predict the exam results of the students.\n",
    "The dataset is taken from [Kaggle](https://www.kaggle.com/datasets/rkiattisak/student-performance-in-mathematics).\n",
    "\n",
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import stat\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the file\n",
    "\n",
    "filename = 'data/exams.csv'\n",
    "\n",
    "print(f'File size: {stat(filename).st_size / 1024} kB.')\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info())\n",
    "display(df.head(10))\n",
    "print(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "We are now going to visualize the data to get a better understanding of it.\n",
    "### Gender repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(df['gender'].unique(), df['gender'].value_counts(), color = 'blue')\n",
    "plt.title('Gender repartition')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parental level of education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 11))\n",
    "plt.bar(df['parental level of education'].unique(), df['parental level of education'].value_counts(), color = 'blue')\n",
    "plt.title('Parental level of education repartition')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['lunch'].unique(), df['lunch'].value_counts(), color = 'blue')\n",
    "plt.title('Lunch repartition')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test preparation course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['test preparation course'].unique(), df['test preparation course'].value_counts(), color = 'blue')\n",
    "plt.title('Taken test preparation course repartition')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "We are now going to preprocess the data to make it ready for the machine learning model.\n",
    "As we have several categorical variables, we will encode them using integers.\n",
    "This will apply to the following columns:\n",
    "- `gender`;\n",
    "- `parental level of education`;\n",
    "- `lunch`;\n",
    "- `test preparation course`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col != 'math score' and col != 'reading score' and col != 'writing score':\n",
    "        dic = {}\n",
    "        i = 0\n",
    "        for k in df[col].unique():\n",
    "            dic[k] = i\n",
    "            i += 1\n",
    "        df[col] = df[col].map(dic)\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.corr(numeric_only=False))\n",
    "plt.figure(figsize=(13,10))\n",
    "sns.heatmap(df.corr(), cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation heatmap of the dataseet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation matrix shows that the explanatory variables are not correlated with each other.\n",
    "We'll have to keep all of them in the model.\n",
    "Also, the targets variables are not correlated with the explanatory variables, which could be a problem.\n",
    "PCA will indeed be useless in this case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x = 'math score', data = df.drop(['reading score', 'writing score'], axis = 1))\n",
    "plt.title('Math score boxplot')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x = 'reading score', data = df.drop(['math score', 'writing score'], axis = 1))\n",
    "plt.title('Reading score boxplot')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x = 'writing score', data = df.drop(['math score', 'reading score'], axis = 1))\n",
    "plt.title('Writing score boxplot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem and leaf plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stemgraphic as sg\n",
    "for col in ['math score', 'reading score', 'writing score']:\n",
    "    sg.stem_graphic(df[col], scale = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set\n",
    "We are going to split our dataset into a training set (80%) and a test set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['math score', 'reading score', 'writing score'], axis=1), df[['math score', 'reading score', 'writing score']], test_size=0.2, random_state=42)\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Multiple Linear Regression model on the Training set\n",
    "### Training the model\n",
    "We are now going to train our model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "Now, we are going to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score prediction: {reg.score(X_test, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "We are going to use the `cross_val_score` function to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(reg, df.drop(['math score', 'reading score', 'writing score'], axis = 1), df[['math score', 'reading score', 'writing score']], cv=5)\n",
    "print(f'Cross validation scores: {scores}')\n",
    "print(f'Cross validation mean score: {scores.mean()}')\n",
    "print(f'Cross validation standard deviation: {scores.std()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that our regression model is absolutely not good.\n",
    "We are going to try another model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Decision Tree Regression model on the Training set\n",
    "We are going to train our decision tree regression model on the training set.\n",
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score prediction: {reg.score(X_test, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(reg, df.drop(['math score', 'reading score', 'writing score'], axis = 1), df[['math score', 'reading score', 'writing score']], cv=5)\n",
    "print(f'Cross validation scores: {scores}')\n",
    "print(f'Cross validation mean score: {scores.mean()}')\n",
    "print(f'Cross validation standard deviation: {scores.std()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that our decision tree regression model is absolutely not good.\n",
    "We are now going to try a random forest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Random Forest Regression model on the Training set\n",
    "We are now going to train a random forest regression model on the training set.\n",
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators=150, max_depth=10, random_state=42)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "We are going to use the `score` function to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score prediction: {reg.score(X_test, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(reg, df.drop(['math score', 'reading score', 'writing score'], axis = 1), df[['math score', 'reading score', 'writing score']], cv=5)\n",
    "print(f'Cross validation scores: {scores}')\n",
    "print(f'Cross validation mean score: {scores.mean()}')\n",
    "print(f'Cross validation standard deviation: {scores.std()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are a little bit better than the multiple linear regression model.\n",
    "However, this cannot still be considered as a good model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Support Vector Regression model on the Training set\n",
    "We are now going to train a support vector regression model on the training set.\n",
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "reg = SVR()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "We are going to use the `score` function to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score prediction: {reg.score(X_test, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(reg, df.drop(['math score', 'reading score', 'writing score'], axis = 1), df[['math score', 'reading score', 'writing score']], cv=5)\n",
    "print(f'Cross validation scores: {scores}')\n",
    "print(f'Cross validation mean score: {scores.mean()}')\n",
    "print(f'Cross validation standard deviation: {scores.std()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not better than the random forest regression model.\n",
    "We are goiing to try a boosting model.\n",
    "\n",
    "## Training the XGBoost model on the Training set\n",
    "We are now going to train an XGBoost model on the training set.\n",
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "reg_dict =  {\n",
    "    \"math\" : GradientBoostingRegressor(n_estimators=150, max_depth=10, random_state=42),\n",
    "    \"reading\" : GradientBoostingRegressor(n_estimators=150, max_depth=10, random_state=42),\n",
    "    \"writing\" : GradientBoostingRegressor(n_estimators=150, max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "for col in ['math', 'reading', 'writing']:\n",
    "    reg_dict[col].fit(X_train, y_train[f'{col} score'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['math', 'reading', 'writing']:\n",
    "    print(f'{col.capitalize()} score prediction: {reg_dict[col].score(X_test, y_test[f\"{col} score\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we have a better score than the random forest regression model for the `math score`, but not for the `reading score` and the `writing score`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['math', 'reading', 'writing']:\n",
    "    scores = cross_val_score(reg_dict[col], df.drop(['math score', 'reading score', 'writing score'], axis = 1), df[[f'{col} score']], cv=5)\n",
    "    print(f'{col.capitalize()} score cross validation scores: {scores}')\n",
    "    print(f'{col.capitalize()} score cross validation mean score: {scores.mean()}')\n",
    "    print(f'{col.capitalize()} score cross validation standard deviation: {scores.std()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not better than the random forest regression model.\n",
    "We are going to try a neural network.\n",
    "\n",
    "## Training the Artificial Neural Network on the Training set\n",
    "We are now going to train an artificial neural network on the training set.\n",
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
